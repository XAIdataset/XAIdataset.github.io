---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults
title: "Saliency-Bench: A Comprehensive Benchmark for Evaluating Visual Explanations"
excerpt: "This is the description of the project"
layout: default
description: "Saliency-Bench is a collection of 8 image datasets that covers a variety of fields along with fidelity and alignment-based evaluation methods. 
It aims to provide standardized benchmarks for visual explanation of Explainable AI (XAI). 
You can download these datasets from this website, and we also provide a user-friendly API with a quick start guide. 
If you want to learn more, please consider read our paper on Arxiv. "
---

## Motivation
Explainable AI (XAI) has gained significant attention for providing insights into the decision-making processes of deep learning models, particularly for image classification tasks through visual explanations visualized by saliency maps. Despite their success, challenges remain due to the lack of annotated datasets and standardized evaluation pipelines. In this paper, we introduce Saliency-Bench, a novel benchmark suite designed to evaluate visual explanations generated by saliency methods across multiple datasets. We curated, constructed, and annotated eight datasets, each covering diverse tasks such as scene classification, cancer diagnosis, object classification, and action classification, with corresponding ground-truth explanations. The benchmark includes a standardized and unified evaluation pipeline for assessing faithfulness and alignment of the visual explanation, providing a holistic visual explanation performance assessment. We benchmark these eight datasets with widely used saliency methods on different image classifier architec-
tures to evaluate explanation quality. Additionally, we developed an easy-to-use API for automating the evaluation pipeline, from data accessing, and data loading, to result evaluation. 

<!-- The rise of deep learning algorithms has led to significant advancements in computer vision tasks, 
but their "black box" nature has raised concerns regarding interpretability. 
Explainable AI (XAI) has emerged as a critical area of research aiming to open this "black box", 
and shed light on the decision-making process of AI models. 
Visual explanations, as a subset of Explainable Artificial Intelligence (XAI), 
provide intuitive insights into the decision-making processes of AI models handling visual data by highlighting influential areas in an input image. 
Despite extensive research conducted on visual explanations, 
the availability of corresponding real-world datasets with ground truth explanations is scarce in the context of image data. 
To bridge this gap, we introduce an XAI Benchmark comprising a datasets collection from diverse topics 
that provide both class labels and corresponding explanation annotations for images. 
We have processed data from diverse domains to align with our unified visual explanation framework. 
We introduce a comprehensive Visual Explanation pipeline, 
which integrates data loading, preprocessing, experimental setup, and model evaluation processes. 
This structure enables researchers to conduct fair comparisons of various visual explanation techniques. 
In addition, we provide a comprehensive review of over 10 evaluation methods for visual explanation to assist researchers in effectively utilizing our dataset collection. 
To further assess the performance of existing visual explanation methods, we conduct experiments on selected datasets using various evaluation metrics. 
We envision this benchmark could facilitate the advancement of visual explanation models. -->

<img src="/images/overview.png" alt="Overview">
Overview of XAI Benchmark for Visual Explanation

<img src="/images/examples.png" alt="Examples">
Examples of images and human explanation annotations from our published dataset collection for four selected datasets.

## Citation
Please consider cite us: 

> Zhang, Yifei, et al. "XAI Benchmark for Visual Explanation." arXiv preprint arXiv:2310.08537 (2023).

**BibTex**: 
```
@misc{zhang2023xai,
      title={XAI Benchmark for Visual Explanation}, 
      author={Yifei Zhang and Siyi Gu and James Song and Bo Pan and Liang Zhao},
      year={2023},
      eprint={2310.08537},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
